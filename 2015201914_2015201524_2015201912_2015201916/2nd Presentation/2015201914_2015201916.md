# AI Car第二次展示报告
## 实现的功能
- 将手机摄像头捕捉到的影像实时传输到电脑
- 图像识别
- 目标跟踪
- AI Car捕捉移动中小鼠
- 画出小鼠的运动轨迹
# 功能介绍
功能   | 实现效果
---------|------------
实时影像传输   | 使用DroidCam将手机摄像头画面传输到网页，再使用爬虫爬取画面储存到本地
蓝牙指令发送 | 使用python包pybluez操控电脑通过蓝牙发送指令
运动小鼠追踪 | 使用hog+svm图像识别算法识别出画面中的小鼠
运动轨迹标注 | 根据拍摄到的视频，自动判断出运动的物体并对其进行相应的像素标注与运动方向的展示


## 算法介绍
### 图像识别算法
#### HOG + SVM 算法
HOG：方向梯度直方图（HOG,Histogram of Gradient）
- 目前在行人识别领域应用最为广泛
- 是一个局部特征，需要结合图像分割使用
- 通过局部cell操作再组合使得对几何和光学形变适应能力强

![](http://upload-images.jianshu.io/upload_images/8920871-927771cc50931a53.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)


#### HOG算法的原理和具体实现步骤：

1. 灰度化（由于颜色在此算法中的作用不大，将图像转化为灰度图）
![](https://latex.codecogs.com/gif.latex?Y=0.299R+0.587G+0.114B)
2. 采用Gamma校正法对输入图像进行颜色空间的标准化（归一化）
目的是调节图像的对比度，降低图像局部的阴影和光照变化所造成的影响，同时可以抑制噪音的干扰
 ![](https://latex.codecogs.com/gif.latex?I(x,y)=I(x,y)^\gamma)
 一般gamma取0.4或0.45较为符合适中视觉光照。
3. 计算图像每个像素的梯度（包括大小和方向）；主要是为了捕获轮廓信息，同时进一步弱化光照的干扰。

![](https://latex.codecogs.com/gif.latex?G_x(x,y)=H(x+1,y)-H(x-1,y))

![](https://latex.codecogs.com/gif.latex?G_x(x,y)=H(x,y+1)-H(x,y-1))

式中![](https://latex.codecogs.com/gif.latex?G_x(x,y))，![](https://latex.codecogs.com/gif.latex?G_y(x,y))，H(x,y)分别表示输入图像中像素点（x，y）处的水平方向梯度、垂直方向梯度和像素点。像素点（x，y）处的梯度幅值和梯度方向分别为：

![](https://latex.codecogs.com/gif.latex?G(x,y)=\sqrt{G_x(x,y)^2+G_y(x,y)^2})

![](https://latex.codecogs.com/gif.latex?\alpha(x,y)=tan^{-1}(\frac{G_x(x,y)}{G_y(x,y)}))

```py
height, width = img.shape
gradient_values_x = cv2.Sobel(img, cv2.CV_64F, 1, 0, ksize=5)
gradient_values_y = cv2.Sobel(img, cv2.CV_64F, 0, 1, ksize=5)
gradient_magnitude = cv2.addWeighted(gradient_values_x, 0.5, gradient_values_y, 0.5, 0)
gradient_angle = cv2.phase(gradient_values_x, gradient_values_y, angleInDegrees=True)
print gradient_magnitude.shape, gradient_angle.shape
```
4. 将图像划分成小cells（例如8*8像素/cell）
5. 加权投影法（weighted voting）每个cell的梯度直方图，即可形成每个cell的descriptor。
具体方法：对每个cell的360度划分bin, 统计在每个梯度方向bin中的像素点个数进行投票，投票权重为改点的梯度幅值（也可使用与幅值相关的函数，但经试验效果没有简单幅值好）
![](http://upload-images.jianshu.io/upload_images/8920871-22c2decde8e2c1c3.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)
具体操作中，往往使用三线性差值，即将当前像素的梯度方向大小、像素在cell中的x坐标与y坐标这三个值来作为插值权重，而被用来插入的值为像素的梯度幅值。采用三线性插值的好处在于：避免了梯度方向直方图在cell边界和梯度方向量化的bin边界处的突然变化。
```py
cell_size = 8
bin_size = 8
angle_unit = 360 / bin_size
gradient_magnitude = abs(gradient_magnitude)
cell_gradient_vector = np.zeros((height / cell_size, width / cell_size, bin_size))

print cell_gradient_vector.shape

def cell_gradient(cell_magnitude, cell_angle):
    orientation_centers = [0] * bin_size
    for k in range(cell_magnitude.shape[0]):
        for l in range(cell_magnitude.shape[1]):
            gradient_strength = cell_magnitude[k][l]
            gradient_angle = cell_angle[k][l]
            min_angle = int(gradient_angle / angle_unit)%8
            max_angle = (min_angle + 1) % bin_size
            mod = gradient_angle % angle_unit
            orientation_centers[min_angle] += (gradient_strength * (1 - (mod / angle_unit)))
            orientation_centers[max_angle] += (gradient_strength * (mod / angle_unit))
    return orientation_centers


for i in range(cell_gradient_vector.shape[0]):
    for j in range(cell_gradient_vector.shape[1]):
        cell_magnitude = gradient_magnitude[i * cell_size:(i + 1) * cell_size,
                         j * cell_size:(j + 1) * cell_size]
        cell_angle = gradient_angle[i * cell_size:(i + 1) * cell_size,
                     j * cell_size:(j + 1) * cell_size]
        print cell_angle.max()

        cell_gradient_vector[i][j] = cell_gradient(cell_magnitude, cell_angle)
```
6. 将每几个cell组成一个block（例如3*3个cell/block），块内归一化梯度直方图
由于局部光照的变化以及前景-背景对比度的变化，使得梯度强度的变化范围非常大。这就需要对梯度强度做归一化。归一化能够进一步地对光照、阴影和边缘进行压缩。
注意：block之间的是“共享”的，也即是说，一个cell会被多个block“共享”。另外，每个“cell”在被归一化时都是“block”independent，也就是说每个cell在其所属的block中都会被归一化一次，得到一个vector。这就意味着：每一个单元格的特征会以不同的结果多次出现在最后的特征向量中。
这种混叠效应增强了块与块之间的联系，使得局部信息得到连接。
一个block内所有cell的特征descriptor串联起来便得到该block的HOG特征descriptor。
![](http://upload-images.jianshu.io/upload_images/8920871-78939926d4f62701.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)
```py
import math
import matplotlib.pyplot as plt

hog_image= np.zeros([height, width])
cell_gradient = cell_gradient_vector
cell_width = cell_size / 2
max_mag = np.array(cell_gradient).max()
for x in range(cell_gradient.shape[0]):
    for y in range(cell_gradient.shape[1]):
        cell_grad = cell_gradient[x][y]
        cell_grad /= max_mag
        angle = 0
        angle_gap = angle_unit
        for magnitude in cell_grad:
            angle_radian = math.radians(angle)
            x1 = int(x * cell_size + magnitude * cell_width * math.cos(angle_radian))
            y1 = int(y * cell_size + magnitude * cell_width * math.sin(angle_radian))
            x2 = int(x * cell_size - magnitude * cell_width * math.cos(angle_radian))
            y2 = int(y * cell_size - magnitude * cell_width * math.sin(angle_radian))
            cv2.line(hog_image, (y1, x1), (y2, x2), int(255 * math.sqrt(magnitude)))
            angle += angle_gap

plt.imshow(hog_image, cmap=plt.cm.gray)
plt.show()
```
7. 将图像image内的所有block的HOG特征descriptor串联起来就可以得到该image（你要检测的目标）的HOG特征descriptor了。这个就是最终的可供分类使用的特征向量了。

#### 算法步骤
1. 对图片素材进行切割，得到64*64的样本块。含小鼠的positive样本tag为1，噪声背景的negative样本tag为0.
2. 用HOG描述子提取特征后用SVM训练，将训练好的模型存储。后续小车实时拍摄的图片，直接加载训练好的模型进行预测，节省时间，以保证实时性。
3. 以64*64的block在每一张手机拍摄的640*480的图像上移动，移动步长为64像素。
4. 由训练好的SVM训练出每个block是否含有小鼠。将图片分区为左、中、右三个分区，统计出每个分区中含有小鼠的block数量，比较判断后，给图片归类。按照图片归属的“左”、“中”、“右”或“无”类别通过蓝牙发送信号，控制小车左转、直行、右转、持续左转。
（当图片识别未发现老鼠时，小车持续原地左转，直到小车视野范围内出现小鼠，开始追踪）
![](http://upload-images.jianshu.io/upload_images/8920871-3b529c1201c1f321.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)


### 目标跟踪
#### 理论基础
我们的算法是基于光流法的。

光流法是一种图像中物体运动的表达方式，即目标、场景或者摄像机的运动或者任意两者的运动所导致的两帧图像之间目标的运动。其运动的判定方法是判断两张图里面相似像素数据的时域和相关性等来获取图片中的运动关系。想要让这个方法可用，需要在亮度、时间与空间这三方面满足条件，具体而言：
- **亮度**：相邻两帧像素强度不发生变化或者变化较少，这个前提是为了保证算法能够识别出相邻两帧图片中相同物体的像素
- **时间**：相邻两帧之间时间较短，这个前提是为了使得物体运动变化较小，因为相同像素的识别是基于一定是时域与空间的，并且也能适当减少亮度的变化
- **空间**：相邻像素具有相似运动，这个前提是为了使得算法能够找出物体的整体运动方向，对相邻空间的像素点运动方向进行一个聚合操作。

在目标检测中，光流法假设图像中的每个像素点都有一个速度矢量，于是就形成一个运动矢量场，在某一特定时刻，图像上的点就与实际物体上的店一一对应，根据这些速度矢量特征，可以对其进行运动检测。若图中没有运动目标，那么光流矢量在整个图像区域中的变化是连续的，但有运动物体存在时，目标和背景中就有着相对运动，这两者的矢量运动方向必然存在着不同，由此就可以检测出运动物体的对应像素，从而得出运动物体识别结果。
###算法实现
本算法实现是基于论文[Two-Frame Motion Estimation Based on Polynomial Expansion](https://www.researchgate.net/profile/Gunnar_Farnebaeck/publication/225138825_Two-Frame_Motion_Estimation_Based_on_Polynomial_Expansion/links/0c96051ac945bb56de000000/Two-Frame-Motion-Estimation-Based-on-Polynomial-Expansion.pdf)，具体描述如下：

对于每一张图片，都能用如下公式将其表示出来：

![f_{1}(x)=x^{T}A_{1}x+b_{1}^{T}x+c_{1}](https://latex.codecogs.com/gif.latex?f_{1}(x)=x^{T}A_{1}x+b_{1}^{T}x+c_{1})

同理，对于下一帧图片，我们假设其函数表达式不变，只是其中每个像素点发生了位移，那么就得到下式：

![f_{2}(x)=f_{1}(x-d)=(x-d)^{T}A_{1}(x-d)+b_{1}^{T}(x-d)+c_{1}
](https://latex.codecogs.com/gif.latex?f_{2}(x)=f_{1}(x-d)=(x-d)^{T}A_{1}(x-d)+b_{1}^{T}(x-d)+c_{1}
)

拆开合并同次项可以得到：

![f_{2}(x)=x^{T}A_{1}x+(b_{1}^{T}-2A_{1}d)x+d^{T}A_{1}d-b_{1}^{T}d+c_{1}
](https://latex.codecogs.com/gif.latex?f_{2}(x)=x^{T}A_{1}x+(b_{1}^{T}-2A_{1}d)x+d^{T}A_{1}d-b_{1}^{T}d+c_{1})

变换成 ![f_{1}(x)](https://latex.codecogs.com/gif.latex?f_{1}(x)) 的形式可以得到：

![f_{2}(x)=x^{T}A_{2}x+b_{2}^{T}x+c_{2}](https://latex.codecogs.com/gif.latex?f_{2}(x)=x^{T}A_{2}x+b_{2}^{T}x+c_{2})

二次项系数显然没有变化，我们的核心在于一次项系数的改变。由于两式的对应系数要相等，对于一次项系数，我们可以建立如下恒等关系：

![b_{2}=b_{1}-2A_{1}d](https://latex.codecogs.com/gif.latex?b_{2}=b_{1}-2A_{1}d)

化简得：

![2A_{1}d=-(b_{2}-b_{1})](https://latex.codecogs.com/gif.latex?2A_{1}d=-(b_{2}-b_{1}))

对其进行变量替换可以得到：

![A(x)d(x)=\Delta b(x)](https://latex.codecogs.com/gif.latex?A%28x%29d%28x%29%3D%5CDelta%20b%28x%29)

但是，由于实际图像的变化并不完全满足` *f(x)* `的函数关系，因此，我们的目标在于使等式左右两边的差值最小化，具体而言，也就是最小化下式：

![\sum_{\Delta x\in I}\omega (\Delta x)\left \| A(x+\Delta x)d(x)-\Delta b(x+\Delta x) \right \|^{2}](https://latex.codecogs.com/gif.latex?%5Csum_%7B%5CDelta%20x%5Cin%20I%7D%5Comega%20%28%5CDelta%20x%29%5Cleft%20%5C%7C%20A%28x&plus;%5CDelta%20x%29d%28x%29-%5CDelta%20b%28x&plus;%5CDelta%20x%29%20%5Cright%20%5C%7C%5E%7B2%7D)

其中的各项系数均是可以直接从图片信息中计算出来的，对于权重，使用常用的机器学习中的梯度下降法等就可以将其求出来。

在实际过程中，我们并不是对整张图片进行操作，二是每16个像素设置一个采样点，这样使得计算速度大大加快，保证了模型的实时响应性。

## 实现效果
[视频地址](https://github.com/KunlinY/AICar/tree/master/2nd%20Presentation)
## 小组分工
组员 | 参与工作
---- | ----
杨昆霖 | 目标跟踪算法、图像识别
刘珏 | 图像识别
施畅 | 图像识别
## 未来展望
- 实现语音识别功能
- 实现自动化识别物体功能
